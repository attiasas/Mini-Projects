import numpy as np
from sklearn.preprocessing import LabelBinarizer


# == part 1 - forward propagation process ==============================================================
def initialize_parameters(layer_dims):
    """
    :param layer_dims: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened input, layer L is the output softmax)
    :return: a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL).
    """
    network = {}

    for i in range(1, len(layer_dims)):
        layerW = np.random.randn(layer_dims[i], layer_dims[i - 1]) * np.sqrt(2 / layer_dims[i - 1])
        layerB = np.zeros((layer_dims[i], 1))
        network[i] = (layerW, layerB)

    return network

def linear_forward(A, W, b):
    """
    Description: Implement the linear part of a layer's forward propagation.

    :param A: the activations of the previous layer
    :param W: the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    :param b:the bias vector of the current layer (of shape [size of current layer, 1])

    :return:
        Z – the linear component of the activation function (i.e., the value before applying the non-linear function)
        linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute)
    """
    Z = W.dot(A) + b
    linear_cache = (A, W, b)

    return Z, linear_cache

def softmax(Z):
    """
    :param Z: the linear component of the activation function
    :return:
        A – the activations of the layer
        activation_cache – returns Z, which will be useful for the backpropagation
    """
    expZ = np.exp(Z)
    return expZ / expZ.sum(axis=0, keepdims=True), Z

def relu(Z):
    """
    :param Z: the linear component of the activation function
    :return:
        A – the activations of the layer
        activation_cache – returns Z, which will be useful for the backpropagation
    """
    return np.maximum(0, Z), Z

def linear_activation_forward(A_prev, W, B, activation):
    """
    Description:
        Implement the forward propagation for the LINEAR->ACTIVATION layer

    :param A_prev: activations of the previous layer
    :param W: the weights matrix of the current layer
    :param B: the bias vector of the current layer
    :param activation: the activation function to be used (a string, either “softmax” or “relu”)
    :return:
        A – the activations of the current layer
        cache – a joint dictionary containing both linear_cache and activation_cache
    """

    Z, linear_cache = linear_forward(A_prev, W, B)

    if (activation is "softmax"):
        A, activation_cache = softmax(Z)
        return A, (linear_cache, activation_cache)
    else:
        A, activation_cache = relu(Z)
        return A, (linear_cache, activation_cache)

def L_model_forward(X, parameters, use_batchnorm, masks=None):
    """
    Description:
        Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation

    :param X: the data, numpy array of shape (input size, number of examples)
    :param parameters: the initialized W and b parameters of each layer
    :param use_batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation
                          (note that this option needs to be set to “false” in Section 3 and “true” in Section 4).
    :return:
        AL – the last post-activation value
        caches – a list of all the cache objects generated by the linear_forward function
    """

    caches = []

    A = X
    if use_batchnorm:
        A = apply_batchnorm(A)
    if masks is not None:
        mask = np.random.rand(A.shape[0], A.shape[1]) < keep_prob
        masks[0] = mask
        A = np.multiply(A, mask)
        A = A / keep_prob

    for i in range(len(parameters) - 1):

        W, B = parameters[i + 1]

        if use_batchnorm:
            A = apply_batchnorm(A)
        A, cache = linear_activation_forward(A, W, B, "relu")

        if masks is not None:
            # apply dropout
            mask = np.random.rand(A.shape[0], A.shape[1]) < keep_prob
            masks[i + 1] = mask
            A = np.multiply(A, mask)
            A = A / keep_prob



        caches.append(cache)

    W, B = parameters[len(parameters)]
    AL, cache = linear_activation_forward(A, W, B, "softmax")
    caches.append(cache)
    return AL, caches, masks

def compute_cost(AL, Y):
    """
    Description:
        Implement the cost function defined by equation.
        The requested cost function is categorical cross-entropy loss.

    :param AL: probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
    :param Y: the labels vector (i.e. the ground truth)
    :return:
        cost – the cross-entropy cost
    """

    return -np.mean((Y * np.log(AL + 1e-8)).sum(axis=0))

def apply_batchnorm(A):
    """
    Description:
        performs batchnorm on the received activation values of a given layer.

    Input:
        A - the activation values of a given layer

    output:
        NA - the normalized activation values, based on the formula learned in class

    :param A:
    :return:
    """
    epsilon = 1e-6

    meu = A.mean(keepdims=True)
    sigma = A.var(keepdims=True)
    NA = (A - meu) / (np.sqrt(sigma + epsilon))
    return NA
# ======================================================================================================

# == part 2 - backward propagation process =============================================================
def Linear_backward(dZ, cache):
    """
    description:
        Implements the linear part of the backward propagation process for a single layer

    :param dZ: the gradient of the cost with respect to the linear output of the current layer (layer l)
    :param cache: tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
    :return:
        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1),same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """

    A_prev, W, b = cache

    n_samples = dZ.shape[1]

    dW = dZ.dot(A_prev.T) / n_samples
    db = dZ.sum(axis=1, keepdims=True) / n_samples
    dA_prev = W.T.dot(dZ)

    return dA_prev, dW, db

def relu_backward(dA, activation_cache):
    """
    Description:
        Implements backward propagation for a ReLU unit

    :param dA: the post-activation gradient
    :param activation_cache: contains Z (stored during the forward propagation)
    :return:
        dZ – gradient of the cost with respect to Z
    """
    activation_cache[activation_cache <= 0] = 0
    activation_cache[activation_cache > 0] = 1

    return dA * activation_cache

def softmax_backward(dA, activation_cache):
    """
    Description:
        Implements backward propagation for a softmax unit

    :param dA: the post-activation gradient
    :param activation_cache: contains Z (stored during the forward propagation)
    :return:
        dZ – gradient of the cost with respect to Z
    """
    AL, Y = activation_cache

    return AL - Y

def linear_activation_backward(dA, cache, activation):
    """
    Description:
        Implements the backward propagation for the LINEAR->ACTIVATION layer.
        The function first computes dZ and then applies the linear_backward function.

    :param dA: post activation gradient of the current layer
    :param cache: contains both the linear cache and the activations cache
    :param activation: the activation function to be used (a string, either “softmax” or “relu”)
    :return:
        dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW – Gradient of the cost with respect to W (current layer l), same shape as W
        db – Gradient of the cost with respect to b (current layer l), same shape as b
    """
    linear_cache, activation_cache = cache

    if activation == "softmax":
        dZ = softmax_backward(dA, activation_cache)
    elif activation == "relu":
        dZ = relu_backward(dA, activation_cache)

    return Linear_backward(dZ, linear_cache)

def L_model_backward(AL, Y, caches, masks):
    """
    Description:
        Implement the backward propagation process for the entire network.
        the backpropagation for the softmax function should be done only once as only the output layers uses it
        and the RELU should be done iteratively over all the remaining layers of the network.

    :param AL: the probabilities vector, the output of the forward propagation (L_model_forward)
    :param Y: the true labels vector (the "ground truth" - true classifications)
    :param caches: list of caches containing for each layer: a) the linear cache; b) the activation cache
    :return:
        Grads - a dictionary with the gradients
             grads["dA" + str(l)] = ...
             grads["dW" + str(l)] = ...
             grads["db" + str(l)] = ...
    """
    grads = {}

    for i in range(len(caches) - 1, -1, -1):

        if i == len(caches) - 1:
            linear_cache, activation_cache = caches[i]
            dA_prev, dW, db = linear_activation_backward(None, (linear_cache, (AL, Y)), "softmax")
        else:
            dA_prev, dW, db = linear_activation_backward(dA, caches[i], "relu")

        if masks is not None:
            # apply dropout
            dA_prev = np.multiply(dA_prev, masks[i])

        grads["dA" + str(i + 1)] = dA_prev
        grads["dW" + str(i + 1)] = dW
        grads["db" + str(i + 1)] = db

        dA = dA_prev

    return grads

def Update_parameters(parameters, grads, learning_rate):
    """
    Description:
        Updates parameters using gradient descent

    :param parameters: a python dictionary containing the DNN architecture’s parameters
    :param grads: a python dictionary containing the gradients (generated by L_model_backward)
    :param learning_rate: the learning rate used to update the parameters (the “alpha”)
    :return:
        parameters – the updated values of the parameters object provided as input
    """
    layers_count = len(parameters)
    new_parameters = {}

    for i in range(layers_count, 0, - 1):
        W, B = parameters[i]
        dW = grads["dW" + str(i)]
        db = grads["db" + str(i)]

        W = W - learning_rate * dW
        B = B - learning_rate * db

        new_parameters[i] = (W, B)

    return new_parameters
# ======================================================================================================

# == part 3 - train the network and produce predictions ================================================
def split_train_validation(X, Y):
    """
    Split the given data into validation set and train set
    :param X: samples to split
    :param Y: labels of the samples
    :return:
        x_validation - samples of validation set
        y_validation - labels of validation set
        x_train - samples of training set
        y_train - labels of training set
    """

    split_percent = 0.2

    indices = np.random.choice(X.shape[1], int(X.shape[1] * split_percent), replace=False)
    x_validation = X[:, indices]
    y_validation = Y[:, indices]
    mask = np.ones(X.shape[1], dtype=bool)
    mask[indices] = False
    x_train = X[:, mask]
    y_train = Y[:, mask]

    return x_validation, y_validation, x_train, y_train

def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size, use_batchnorm=False, use_dropout=False):
    """
    Description:
        Implements a L-layer neural network. All layers but the last should have the ReLU activation function,
        and the final layer will apply the softmax activation function.
        The size of the output layer should be equal to the number of labels in the data.
        Please select a batch size that enables your code to run well
        (i.e. no memory overflows while still running relatively fast).

        Hint: the function should use the earlier functions in the following order:
        initialize -> L_model_forward -> compute_cost -> L_model_backward -> update parameters

    :param X: the input data, a numpy array of shape (height*width , number_of_examples)
              Comment: since the input is in grayscale we only have height and width,
              otherwise it would have been height*width*3
    :param Y: the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
    :param layers_dims: a list containing the dimensions of each layer, including the input
    :param learning_rate: the learning rate used to update the parameters (the “alpha”)
    :param num_iterations:
    :param batch_size: the number of examples in a single training batch.
    :return:
        parameters – the parameters learnt by the system during the training
                     (the same parameters that were updated in the update_parameters function).
        costs – the values of the cost function (calculated by the compute_cost function).
                One value is to be saved after each 100 training iterations (e.g. 3000 iterations -> 30 values).
    """
    # init
    parameters = initialize_parameters(layers_dims)
    x_validation, y_validation, x_train, y_train = split_train_validation(X, Y)

    # constants
    itrs_between_save_cost = 100
    batches_per_epoch = np.ceil(x_train.shape[1] / batch_size)  # how many batches (iterations) fits in epoch
    itr_no_change_to_stop = int(batches_per_epoch)   # 1 epoch between checks on early stop

    costs = []  # holds the costs that will be saved in learning
    epoch_idx = []  # holds the index of the iteration (in ratio of epochs) that the cost was created

    epoch_count = 1  # counts how many epochs elapsed
    max_acc = 0  # holds the maximum accuracy that validation achieved

    masks = None  # dict for masks (dropout)
    if use_dropout:
        masks = {}

    for itr in range(num_iterations):

        # prepare current batch
        start_idx = (itr * batch_size) % x_train.shape[1]

        if start_idx + batch_size < x_train.shape[1]:
            end_idx = start_idx + batch_size
        else:
            end_idx = x_train.shape[1] - 1
            epoch_count += 1

        batch_x = x_train[:, start_idx:end_idx]
        batch_y = y_train[:, start_idx:end_idx]

        # Forward
        AL, caches, masks = L_model_forward(batch_x, parameters, use_batchnorm, masks)

        # Cost
        cost = compute_cost(AL, batch_y)
        if itr % itrs_between_save_cost == 0:
            epoch_idx.append(itr / batches_per_epoch)
            costs.append(cost)

        # check for early stop on validation
        if itr % itr_no_change_to_stop == 0:
            accuracy = Predict(x_validation, y_validation, parameters, use_batchnorm)
            print("Epoch: ", epoch_count, " iteration: ", itr, " | Cost: ", round(cost, 3), " | Validation Accuracy:",
                  round(accuracy, 3))
            if accuracy > max_acc + 0.2:
                max_acc = accuracy
            else:
                break

        # Backward
        grads = L_model_backward(AL, batch_y, caches, masks)
        # Update
        parameters = Update_parameters(parameters, grads, learning_rate)

    return parameters, (epoch_idx, costs)

def Predict(X, Y, parameters, use_batchnorm):
    """
    Description:
        The function receives an input data and the true labels
        and calculates the accuracy of the trained neural network on the data.

    :param X: the input data, a numpy array of shape (height*width, number_of_examples)
    :param Y: the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
    :param parameters: a python dictionary containing the DNN architecture’s parameters
    :return:
        accuracy – the accuracy measure of the neural net on the provided data
        (i.e. the percentage of the samples for which the correct label receives the hughest confidence score).
        Use the softmax function to normalize the output values.
    """

    AL, caches, masks = L_model_forward(X, parameters, use_batchnorm)

    predicted_Y = np.argmax(AL, axis=0)
    real_Y = np.argmax(Y, axis=0)
    accuracy = (predicted_Y == real_Y).mean()

    return accuracy * 100
# ======================================================================================================

# == part 4 ============================================================================================
from keras.datasets import mnist
import matplotlib.pylab as plt
import time

input_flatten_size = 28 * 28
layers_dims = [input_flatten_size, 20, 7, 5, 10]
# seed = 42
learning_rate = 0.009
num_iterations = 80000
batch_size = 32
keep_prob = 0.85

def get_data():
    """
    Get MNIST data set and prepare (pre-process) it for learning
    * normalize value of samples to the range [0,1]
    * transform categorical labels vector into binaries label matrix
    * transpose data to fit methods parameters shapes
    :return:
        trainData - tuple of training data the stores samples data ( x_train, y_train )
        testData - tuple of testing data the stores samples data ( x_test, y_test )
    """
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    x_train = x_train / 255.
    x_test = x_test / 255.

    x_train = x_train.reshape(x_train.shape[0], input_flatten_size).T
    x_test = x_test.reshape(x_test.shape[0], input_flatten_size).T

    y_train = y_train.reshape(y_train.shape[0], -1)
    y_train = LabelBinarizer().fit_transform(y_train).T
    y_test = y_test.reshape(y_test.shape[0], -1)
    y_test = LabelBinarizer().fit_transform(y_test).T

    return (x_train, y_train), (x_test, y_test)

def fit_predict_analyze(X_train, Y_train, X_test, Y_test, learning_rate, num_iterations, batch_size, use_batchnorm, use_dropout, title):
    """
    Apply all the process [fit -> predict -> print results] for a given data set and model hyper parameters

    :param X_train: Matrix of training samples
    :param Y_train: Vector of training labels
    :param X_test: Matrix of testing samples
    :param Y_test: Vector of testing labels
    :param learning_rate: learning rate of the NN Model
    :param num_iterations: #iterations of batches on the NN Model until early stopping
    :param batch_size: #samples in a single batch of train
    :param use_batchnorm: flag if to use batchnorm in learning, deafult = false
    :param use_dropout:
    :param title:
    """

    # FIT
    start_t = time.time()
    parameters, (epoch_idx, costs) = L_layer_model(X_train, Y_train, layers_dims, learning_rate, num_iterations,
                                                   batch_size, use_batchnorm, use_dropout)
    end_t = time.time()

    # Predict
    train_accuracy = Predict(X_train, Y_train, parameters, use_batchnorm)
    test_accuracy = Predict(X_test, Y_test, parameters, use_batchnorm)

    # Print Results
    print("Fit Time Elapsed: " + str(end_t - start_t))
    print("Train Accuracy: " + str(train_accuracy))
    print("Test Accuracy: " + str(test_accuracy))

    # Plot Costs
    plt.figure()
    plt.title(title)
    plt.plot(epoch_idx, costs)
    plt.xlabel("epoch")
    plt.ylabel("cost")
    plt.show()

(x_train, y_train), (x_test, y_test) = get_data()

# np.random.seed(seed)
fit_predict_analyze(x_train, y_train, x_test, y_test, learning_rate, num_iterations, batch_size, False, False, "No BatchNorm, No Dropout")

# np.random.seed(seed)
fit_predict_analyze(x_train, y_train, x_test, y_test, learning_rate, num_iterations, batch_size, True, False, "With BatchNorm, No Dropout")

# np.random.seed(seed)
fit_predict_analyze(x_train, y_train, x_test, y_test, learning_rate, num_iterations, batch_size, False, True, "No BatchNorm, With Dropout")
